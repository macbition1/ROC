{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1zRjcvJ-a4YxmGXybv7utVHpp8lFJ77JC",
      "authorship_tag": "ABX9TyMpVVRocd1SXItG4bLOtPCk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macbition1/ROC1/blob/main/ROS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgLzVzO_zLZS",
        "outputId": "a4960c20-c70d-4b16-f71e-b6a8ce9cc93f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.0\n",
        "!pip install torchvision==0.8.0\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install scipy==1.5.2\n",
        "!pip install Pillow==6.2.2\n",
        "!pip install numpy==1.17.0\n",
        "!pip install scikit-learn==0.22.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XgtSV4Zq28D3",
        "outputId": "36f15d97-71e0-4944-925a-c547decb30a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.2 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.10.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0\n",
            "Collecting torchvision==0.8.0\n",
            "  Downloading torchvision-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (1.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (7.1.2)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (3.10.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "Successfully installed torchvision-0.8.0\n",
            "Collecting tensorflow-gpu==1.15.0\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 48.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.43.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.13.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=e441ed8f8a07085bae5384bb61d3a9129091789ced7e2a7cae59c7b793fb66ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Collecting scipy==1.5.2\n",
            "  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.5.2) (1.19.5)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.5.2\n",
            "Collecting Pillow==6.2.2\n",
            "  Downloading Pillow-6.2.2-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.2.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-6.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.17.0\n",
            "  Downloading numpy-1.17.0-cp37-cp37m-manylinux1_x86_64.whl (20.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.3 MB 1.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "pywavelets 1.2.0 requires numpy>=1.17.3, but you have numpy 1.17.0 which is incompatible.\n",
            "kapre 0.3.6 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "jaxlib 0.1.71+cuda111 requires numpy>=1.18, but you have numpy 1.17.0 which is incompatible.\n",
            "jax 0.2.25 requires numpy>=1.18, but you have numpy 1.17.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "bokeh 2.3.3 requires pillow>=7.1.0, but you have pillow 6.2.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.22.1\n",
            "  Downloading scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1) (1.17.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.1) (1.5.2)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.6 requires numpy>=1.18.5, but you have numpy 1.17.0 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.1 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data_helper\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "from dataset import Dataset, TestDataset, _dataset_info\n",
        "\n",
        "\n",
        "def get_train_dataloader(args,txt_file):\n",
        "\n",
        "\n",
        "    img_transformer = get_train_transformers(args)\n",
        "    name_train, labels_train = _dataset_info(txt_file)\n",
        "    train_dataset = Dataset(name_train, labels_train, args.path_dataset, img_transformer=img_transformer)\n",
        "    loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_val_dataloader(args,txt_file):\n",
        "\n",
        "    names, labels = _dataset_info(txt_file)\n",
        "    img_tr = get_test_transformer(args)\n",
        "    test_dataset = TestDataset(names, labels,args.path_dataset, img_transformer=img_tr)\n",
        "    loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "def get_train_transformers(args):\n",
        "\n",
        "    img_tr = [transforms.RandomResizedCrop((int(args.image_size), int(args.image_size)), (args.min_scale, args.max_scale))]\n",
        "\n",
        "    if args.jitter > 0.0:\n",
        "        img_tr.append(transforms.ColorJitter(brightness=args.jitter, contrast=args.jitter, saturation=args.jitter, hue=min(0.5, args.jitter)))\n",
        "    if args.random_grayscale:\n",
        "        img_tr.append(transforms.RandomGrayscale(args.random_grayscale))\n",
        "\n",
        "    img_tr = img_tr + [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
        "\n",
        "    return transforms.Compose(img_tr)\n",
        "\n",
        "\n",
        "def get_test_transformer(args):\n",
        "\n",
        "    img_tr = [transforms.Resize((args.image_size, args.image_size)), transforms.ToTensor(),\n",
        "              transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
        "\n",
        "    return transforms.Compose(img_tr)"
      ],
      "metadata": {
        "id": "-dJEvsht5Mv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "acbdac54-5d4c-4a02-b633-0043bfe06dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-255b3e820f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTestDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dataset_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "from random import random\n",
        "import random\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "\n",
        "def _dataset_info(txt_labels):\n",
        "    with open(txt_labels, 'r') as f:\n",
        "        images_list = f.readlines()\n",
        "\n",
        "    file_names = []\n",
        "    labels = []\n",
        "    for row in images_list:\n",
        "        row = row.split(' ')\n",
        "        file_names.append(row[0])\n",
        "        labels.append(int(row[1]))\n",
        "\n",
        "    return file_names, labels\n",
        "\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, names, labels, path_dataset,img_transformer=None):\n",
        "        self.data_path = path_dataset\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self._image_transformer = img_transformer\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "\n",
        "        return img, int(self.labels[index]), img_rot, index_rot\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "\n",
        "\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self, names, labels, path_dataset,img_transformer=None):\n",
        "        self.data_path = path_dataset\n",
        "        self.names = names\n",
        "        self.labels = labels\n",
        "        self._image_transformer = img_transformer\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "\n",
        "        return img, int(self.labels[index]), img_rot, index_rot\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n"
      ],
      "metadata": {
        "id": "rbQ_l5Aw5Zis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eval_target\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import random\n",
        "\n",
        "\n",
        "#### Implement the evaluation on the target for the known/unknown separation\n",
        "\n",
        "def evaluation(args,feature_extractor,rot_cls,target_loader_eval,device):\n",
        "\n",
        "    feature_extractor.eval()\n",
        "    rot_cls.eval()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for it, (data,class_l,data_rot,rot_l) in enumerate(target_loader_eval):\n",
        "            data, class_l, data_rot, rot_l = data.to(device), class_l.to(device), data_rot.to(device), rot_l.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    auroc = roc_auc_score(ground_truth,normality_score)\n",
        "    print('AUROC %.4f' % auroc)\n",
        "\n",
        "    # create new txt files\n",
        "    rand = random.randint(0,100000)\n",
        "    print('Generated random number is :', rand)\n",
        "\n",
        "    # This txt files will have the names of the source images and the names of the target images selected as unknown\n",
        "    target_unknown = open('new_txt_list/' + args.source + '_known_' + str(rand) + '.txt','w')\n",
        "\n",
        "    # This txt files will have the names of the target images selected as known\n",
        "    target_known = open('new_txt_list/' + args.target + '_known_' + str(rand) + '.txt','w')\n",
        "\n",
        "\n",
        "\n",
        "    print('The number of target samples selected as known is: ',number_of_known_samples)\n",
        "    print('The number of target samples selected as unknown is: ', number_of_unknown_samples)\n",
        "\n",
        "    return rand"
      ],
      "metadata": {
        "id": "FQlHCcCE5mdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main.py\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "import data_helper\n",
        "from resnet import resnet18_feat_extractor, Classifier\n",
        "\n",
        "from step1_KnownUnknownSep import step1\n",
        "from eval_target import evaluation\n",
        "import numpy as npy\n",
        "\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# from step2_SourceTargetAdapt import step2\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Script to launch training\",\n",
        "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "\n",
        "    parser.add_argument(\"--source\", default='Art', help=\"Source name\")\n",
        "    parser.add_argument(\"--target\", default='Clipart', help=\"Target name\")\n",
        "    parser.add_argument(\"--n_classes_known\", type=int, default=45, help=\"Number of known classes\")\n",
        "    parser.add_argument(\"--n_classes_tot\", type=int, default=65, help=\"Number of unknown classes\")\n",
        "\n",
        "    # dataset path\n",
        "    parser.add_argument(\"--path_dataset\", default=\"./data\", help=\"Path where the Office-Home dataset is located\")\n",
        "\n",
        "    # data augmentation\n",
        "    parser.add_argument(\"--min_scale\", default=0.8, type=float, help=\"Minimum scale percent\")\n",
        "    parser.add_argument(\"--max_scale\", default=1.0, type=float, help=\"Maximum scale percent\")\n",
        "    parser.add_argument(\"--jitter\", default=0.4, type=float, help=\"Color jitter amount\")\n",
        "    parser.add_argument(\"--random_grayscale\", default=0.1, type=float,help=\"Randomly greyscale the image\")\n",
        "\n",
        "    # training parameters\n",
        "    parser.add_argument(\"--image_size\", type=int, default=222, help=\"Image size\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"Batch size\")\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=0.001, help=\"Learning rate\")\n",
        "\n",
        "    parser.add_argument(\"--epochs_step1\", type=int, default=10, help=\"Number of epochs of step1 for known/unknown separation\")\n",
        "    parser.add_argument(\"--epochs_step2\", type=int, default=10, help=\"Number of epochs of step2 for source-target adaptation\")\n",
        "\n",
        "    parser.add_argument(\"--train_all\", type=bool, default=True, help=\"If true, all network weights will be trained\")\n",
        "\n",
        "    parser.add_argument(\"--weight_RotTask_step1\", type=float, default=0.5, help=\"Weight for the rotation loss in step1\")\n",
        "    parser.add_argument(\"--weight_RotTask_step2\", type=float, default=0.5, help=\"Weight for the rotation loss in step2\")\n",
        "    parser.add_argument(\"--threshold\", type=float, default=0.5, help=\"Threshold for the known/unkown separation\")\n",
        "\n",
        "    # tensorboard logger\n",
        "    parser.add_argument(\"--tf_logger\", type=bool, default=True, help=\"If true will save tensorboard compatible logs\")\n",
        "    parser.add_argument(\"--folder_name\", default=None, help=\"Used by the logger to save logs\")\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # initialize the network with a number of classes equals to the number of known classes + 1 (the unknown class, trained only in step2)\n",
        "        self.feature_extractor = resnet18_feat_extractor()\n",
        "        self.obj_classifier = Classifier(512,self.args.n_classes_known+1)\n",
        "        self.rot_classifier = Classifier(512*2,4)\n",
        "\n",
        "        self.feature_extractor = self.feature_extractor.to(self.device)\n",
        "        self.obj_cls = self.obj_classifier.to(self.device)\n",
        "        self.rot_cls = self.rot_classifier.to(self.device)\n",
        "\n",
        "        source_path_file = 'txt_list/'+args.source+'_known.txt'\n",
        "        self.source_loader = data_helper.get_train_dataloader(args,source_path_file)\n",
        "        \n",
        "        \n",
        "        target_path_file = 'txt_list/' + args.target + '.txt'\n",
        "        self.target_loader_train = data_helper.get_val_dataloader(args,target_path_file)\n",
        "        self.target_loader_eval = data_helper.get_val_dataloader(args,target_path_file)\n",
        "\n",
        "        print(\"Source: \",self.args.source,\" Target: \",self.args.target)\n",
        "        print(\"Dataset size: source %d, target %d\" % (len(self.source_loader.dataset), len(self.target_loader_train.dataset)))\n",
        "\n",
        "    def do_training(self):\n",
        "\n",
        "        # just check if final training parameters are saved somewhere. If so, so not train again\n",
        "\n",
        "        if not os.path.isfile(\"./feature_extractor_params.pt\") and not os.path.isfile(\"./rot_cls_params.pt\"):\n",
        "            print('Step 1 --------------------------------------------')\n",
        "            step1(self.args,self.feature_extractor,self.rot_cls,self.obj_cls,self.source_loader,self.device)\n",
        "\n",
        "        print('Target - Evaluation -- for known/unknown separation')\n",
        "\n",
        "        # if params are already computed, load the model and procede with its evaluation\n",
        "\n",
        "        self.rot_cls.load_state_dict(torch.load(\"./feature_extractor_params.pt\"), strict=False)\n",
        "        self.feature_extractor.load_state_dict(torch.load(\"./rot_cls_params.pt\"), strict=False)\n",
        "\n",
        "        rand = evaluation(self.args,self.feature_extractor,self.rot_cls,self.target_loader_eval,self.device)\n",
        "\n",
        "        # new dataloaders\n",
        "        source_path_file = 'new_txt_list/' + self.args.source + '_known_'+str(rand)+'.txt'\n",
        "        self.source_loader = data_helper.get_train_dataloader(self.args,source_path_file)\n",
        "\n",
        "        target_path_file = 'new_txt_list/' + self.args.target + '_known_' + str(rand) + '.txt'\n",
        "        self.target_loader_train = data_helper.get_train_dataloader(self.args,target_path_file)\n",
        "        self.target_loader_eval = data_helper.get_val_dataloader(self.args,target_path_file)\n",
        "\n",
        "        \"\"\"print('Step 2 --------------------------------------------')\n",
        "        step2(self.args,self.feature_extractor,self.rot_cls,self.obj_cls,self.source_loader,self.target_loader_train,self.target_loader_eval,self.device)\"\"\"\n",
        "\n",
        "def main():\n",
        "    args = get_args()\n",
        "    trainer = Trainer(args)\n",
        "    trainer.do_training()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    main()"
      ],
      "metadata": {
        "id": "-9AWmiM9-s5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer_helper\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "def get_optim_and_scheduler(feature_extractor,rot_cls,obj_cls, epochs, lr, train_all):\n",
        "\n",
        "    if train_all:\n",
        "        params = list(feature_extractor.parameters()) + list(rot_cls.parameters()) + list(obj_cls.parameters())\n",
        "    else:\n",
        "        params = list(rot_cls.parameters()) + list(obj_cls.parameters())\n",
        "\n",
        "    optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, lr=lr)\n",
        "    step_size = int(epochs * .8)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n",
        "\n",
        "\n",
        "    print(\"Step size: %d\" % step_size)\n",
        "    return optimizer, scheduler"
      ],
      "metadata": {
        "id": "ef09_UrY5zt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reset\n",
        "from torch import nn\n",
        "from torch.utils import model_zoo\n",
        "from torchvision.models.resnet import BasicBlock, model_urls, Bottleneck\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def is_patch_based(self):\n",
        "        return False\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self,input_size,classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.class_classifier = nn.Linear(input_size, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.class_classifier(x)\n",
        "\n",
        "\n",
        "\n",
        "def resnet18_feat_extractor():\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "    model.load_state_dict(model_zoo.load_url(model_urls['resnet18']), strict=False)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "QdIpIJn555YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step1\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from optimizer_helper import get_optim_and_scheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "#### Implement Step1\n",
        "\n",
        "\"\"\"def _do_epoch(args,feature_extractor,rot_cls,obj_cls,dataloaders,optimizer,device,phase):\n",
        "    \n",
        "    if phase == \"train\":\n",
        "        print(\"TRAINING MODE\")\n",
        "        feature_extractor.train()\n",
        "        obj_cls.train()\n",
        "        rot_cls.train()\n",
        "    else:\n",
        "        print(\"VALIDATION MODE\")\n",
        "        feature_extractor.eval()\n",
        "        obj_cls.eval()\n",
        "        rot_cls.eval()\n",
        "\n",
        "    running_img_corrects = 0\n",
        "    running_rot_corrects = 0\n",
        "\n",
        "    for i, (imgs, lbls, rot_imgs, rot_lbls) in tqdm(enumerate(dataloaders[phase])):\n",
        "        imgs = imgs.to(device)\n",
        "        lbls = lbls.to(device)\n",
        "        rot_imgs = rot_imgs.to(device)\n",
        "        rot_lbls = rot_lbls.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(phase == \"train\"):\n",
        "            imgs_out = feature_extractor(imgs)\n",
        "            imgs_predictions = obj_cls(imgs_out)\n",
        "            \n",
        "            rot_out = feature_extractor(rot_imgs)\n",
        "            rot_predictions = rot_cls(torch.cat((rot_out, imgs_out), dim=1))\n",
        "            \n",
        "            _, imgs_preds = torch.max(imgs_predictions, 1)\n",
        "            _, rot_preds = torch.max(rot_predictions, 1)\n",
        "\n",
        "            # compute loss\n",
        "\n",
        "            img_loss = nn.CrossEntropyLoss(imgs_predictions, lbls)\n",
        "            rot_loss = nn.CrossEntropyLoss(rot_predictions, rot_lbls)\n",
        "\n",
        "            loss = img_loss + args.weight_RotTask_step1*rot_loss\n",
        "\n",
        "            if phase == \"train\":\n",
        "                    # compute gradient + update params\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # statistics\n",
        "            # raccolgo in ciascun batch la loss per quel batch e quanti elementi sono stati\n",
        "            # classificati correttamente\n",
        "\n",
        "        #running_loss += loss.item() * imgs.size(0)\n",
        "        running_img_corrects += torch.sum(imgs_preds == lbls.data)\n",
        "        running_rot_corrects += torch.sum(rot_preds == rot_lbls.data)\n",
        "\n",
        "    #class_loss = running_loss / dataset_sizes[phase]\n",
        "    img_acc = (running_img_corrects.double() / len(dataloaders[phase].dataset)) * 100\n",
        "    rot_acc = (running_rot_corrects.double() / len(dataloaders[phase].dataset)) * 100\n",
        "\n",
        "    return img_loss, img_acc, rot_loss, rot_acc\"\"\"\n",
        "\n",
        "def _do_epoch(args,feature_extractor,rot_cls,obj_cls,source_loader,optimizer,device,criterion):\n",
        "    \n",
        "    feature_extractor.train()\n",
        "    obj_cls.train()\n",
        "    rot_cls.train()\n",
        "    \n",
        "    running_img_corrects = 0\n",
        "    running_rot_corrects = 0\n",
        "\n",
        "    for i, (imgs, lbls, rot_imgs, rot_lbls) in tqdm(enumerate(source_loader)):\n",
        "        imgs = imgs.to(device)\n",
        "        lbls = lbls.to(device)\n",
        "        rot_imgs = rot_imgs.to(device)\n",
        "        rot_lbls = rot_lbls.to(device)\n",
        "\n",
        "        # forward\n",
        "        imgs_out = feature_extractor(imgs)\n",
        "        imgs_predictions = obj_cls(imgs_out)\n",
        "\n",
        "        rot_out = feature_extractor(rot_imgs)\n",
        "        rot_predictions = rot_cls(torch.cat((rot_out, imgs_out), dim=1))\n",
        "\n",
        "        _, imgs_preds = torch.max(imgs_predictions, 1)\n",
        "        _, rot_preds = torch.max(rot_predictions, 1)\n",
        "\n",
        "        # compute loss\n",
        "\n",
        "        img_loss = criterion(imgs_predictions, lbls)\n",
        "        rot_loss = criterion(rot_predictions, rot_lbls)\n",
        "\n",
        "        loss = img_loss + args.weight_RotTask_step1*rot_loss\n",
        "\n",
        "        # compute gradient + update params\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #statistics\n",
        "        running_img_corrects += torch.sum(imgs_preds == lbls.data)\n",
        "        running_rot_corrects += torch.sum(rot_preds == rot_lbls.data)\n",
        "\n",
        "    img_acc = (running_img_corrects.double() / len(source_loader.dataset)) * 100\n",
        "    rot_acc = (running_rot_corrects.double() / len(source_loader.dataset)) * 100\n",
        "\n",
        "    return img_loss, img_acc, rot_loss, rot_acc\n",
        "\n",
        "def step1(args,feature_extractor,rot_cls,obj_cls,source_loader,device):\n",
        "    optimizer, scheduler = get_optim_and_scheduler(feature_extractor,rot_cls,obj_cls, args.epochs_step1, args.learning_rate, args.train_all)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(args.epochs_step1):\n",
        "        print('Epoch: ',epoch)\n",
        "        class_loss, acc_cls, rot_loss, acc_rot = _do_epoch(args,feature_extractor,rot_cls,obj_cls,source_loader,optimizer,device,criterion)\n",
        "        print(\"Class Loss %.4f, Class Accuracy %.4f,Rot Loss %.4f, Rot Accuracy %.4f\" % (class_loss.item(),acc_cls,rot_loss.item(), acc_rot))\n",
        "        scheduler.step()\n",
        "    \n",
        "    torch.save(feature_extractor.state_dict(), \"./feature_extractor_params.pt\")\n",
        "    torch.save(rot_cls.state_dict(), \"./rot_cls_params.pt\")\n",
        "    torch.save(obj_cls.state_dict(), \"./obj_cls_params.pt\")"
      ],
      "metadata": {
        "id": "DfUfmF9e-21m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step2_SouceTargetAdaption\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from optimizer_helper import get_optim_and_scheduler\n",
        "from itertools import cycle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#### Implement Step2\n",
        "\n",
        "def _do_epoch(args,feature_extractor,rot_cls,obj_cls,source_loader,target_loader_train,target_loader_eval,optimizer,device):\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    feature_extractor.train()\n",
        "    obj_cls.train()\n",
        "    rot_cls.train()\n",
        "\n",
        "    target_loader_train = cycle(target_loader_train)\n",
        "\n",
        "    for it, (data_source, class_l_source, _, _) in enumerate(source_loader):\n",
        "\n",
        "        (data_target, _, data_target_rot, rot_l_target) = next(target_loader_train)\n",
        "\n",
        "        data_source, class_l_source  = data_source.to(device), class_l_source.to(device)\n",
        "        data_target, data_target_rot, rot_l_target  = data_target.to(device), data_target_rot.to(device), rot_l_target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        class_loss = ....\n",
        "        rot_loss = ....\n",
        "\n",
        "        loss = class_loss + args.weight_RotTask_step2*rot_loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        _, cls_pred = ...\n",
        "        _, rot_pred = ...\n",
        "\n",
        "    acc_cls = ...\n",
        "    acc_rot = ...\n",
        "\n",
        "    print(\"Class Loss %.4f, Class Accuracy %.4f,Rot Loss %.4f, Rot Accuracy %.4f\" % (class_loss.item(), acc_cls, rot_loss.item(), acc_rot))\n",
        "\n",
        "\n",
        "    #### Implement the final evaluation step, computing OS*, UNK and HOS\n",
        "    feature_extractor.eval()\n",
        "    obj_cls.eval()\n",
        "    rot_cls.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for it, (data, class_l,_,_) in enumerate(target_loader_eval):\n",
        "\n",
        "\n",
        "\n",
        "def step2(args,feature_extractor,rot_cls,obj_cls,source_loader,target_loader_train,target_loader_eval,device):\n",
        "    optimizer, scheduler = get_optim_and_scheduler(feature_extractor,rot_cls,obj_cls, args.epochs_step2, args.learning_rate, args.train_all)\n",
        "\n",
        "\n",
        "    for epoch in range(args.epochs_step2):\n",
        "        _do_epoch(args,feature_extractor,rot_cls,obj_cls,source_loader,target_loader_train,target_loader_eval,optimizer,device)\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "BCyXfb8g6MwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}